{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gym,torch,torch.nn.functional as F,numpy as np,matplotlib.pyplot as plt,rl_utils\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PolicyNet(torch.nn.Module):\n",
    "    def __init__(self,state_dim,hidden_dim,action_dim):\n",
    "        super(PolicyNet,self).__init__()\n",
    "        # define input layer\n",
    "        self.fc1=torch.nn.Linear(state_dim,hidden_dim)\n",
    "        # define output layer\n",
    "        self.fc2=torch.nn.Linear(hidden_dim,action_dim)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # define forward pass\n",
    "        x = F.relu(self.fc1(x))\n",
    "        return F.softmax(self.fc2(x),dim=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class REINFORCE:\n",
    "    def __init__(self, state_dim, hidden_dim, action_dim, learning_rate, gamma, device):\n",
    "        self.policy_net = PolicyNet(state_dim, hidden_dim, action_dim).to(device)# 初始化策略网络\n",
    "        self.optimizer = torch.optim.Adam(self.policy_net.parameters(), lr=learning_rate)# 初始化优化器\n",
    "        self.gamma = gamma\n",
    "        self.device = device\n",
    "\n",
    "    def take_action(self, state):\n",
    "        state = torch.tensor([state], dtype=torch.float).to(self.device)\n",
    "        probs = self.policy_net(state)\n",
    "        action_dist = torch.distributions.Categorical(probs)\n",
    "        action = action_dist.sample()\n",
    "        return action.item()\n",
    "\n",
    "    def update(self, transitions_dict):\n",
    "        # 从字典中取出所有的训练数据\n",
    "        reward_list = transitions_dict['rewards']\n",
    "        action_list = transitions_dict['actions']\n",
    "        state_list = transitions_dict['states']\n",
    "\n",
    "        G = 0\n",
    "        self.optimizer.zero_grad()\n",
    "        for i in reversed(range(len(reward_list))):\n",
    "            # 计算每一步的奖励\n",
    "            reward = reward_list[i]\n",
    "            state = torch.tensor([state_list[i]], dtype=torch.float).to(self.device)\n",
    "            action = torch.tensor([action_list[i]]).view(1, -1).to(self.device)\n",
    "            log_prob = torch.log(self.policy_net(state)).gather(1, action)\n",
    "            G = reward + self.gamma * G\n",
    "            # 策略网络的损失函数\n",
    "            loss = -log_prob * G\n",
    "            loss.backward()\n",
    "        self.optimizer.step()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\vnstudio\\lib\\site-packages\\gym\\envs\\registration.py:556: UserWarning: \u001b[33mWARN: The environment CartPole-v0 is out of date. You should consider upgrading to version `v1`.\u001b[0m\n",
      "  f\"The environment {id} is out of date. You should consider \"\n",
      "Iteration 0:   0%|          | 0/100 [00:00<?, ?it/s]c:\\vnstudio\\lib\\site-packages\\ipykernel_launcher.py:9: UserWarning: Creating a tensor from a list of numpy.ndarrays is extremely slow. Please consider converting the list to a single numpy.ndarray with numpy.array() before converting to a tensor. (Triggered internally at  C:\\actions-runner\\_work\\pytorch\\pytorch\\builder\\windows\\pytorch\\torch\\csrc\\utils\\tensor_new.cpp:204.)\n",
      "  if __name__ == '__main__':\n",
      "Iteration 0:   0%|          | 0/100 [00:00<?, ?it/s]\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "expected sequence of length 4 at dim 2 (got 0)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-4-5d68b34b5268>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     21\u001b[0m             \u001b[0mdone\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mFalse\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     22\u001b[0m             \u001b[1;32mwhile\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0mdone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 23\u001b[1;33m                 \u001b[0maction\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0magent\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtake_action\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mstate\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     24\u001b[0m                 \u001b[0mnext_state\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mreward\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdone\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0m_\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0menv\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0maction\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     25\u001b[0m                 \u001b[0mtransition_dict\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'states'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mstate\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-3-d405e88da110>\u001b[0m in \u001b[0;36mtake_action\u001b[1;34m(self, state)\u001b[0m\n\u001b[0;32m      7\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      8\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mtake_action\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mstate\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 9\u001b[1;33m         \u001b[0mstate\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtensor\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mstate\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mdtype\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfloat\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mto\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdevice\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     10\u001b[0m         \u001b[0mprobs\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpolicy_net\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mstate\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     11\u001b[0m         \u001b[0maction_dist\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdistributions\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mCategorical\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mprobs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mValueError\u001b[0m: expected sequence of length 4 at dim 2 (got 0)"
     ]
    }
   ],
   "source": [
    "lr = 1e-3\n",
    "num_episodes = 1000\n",
    "hidden_dim = 128\n",
    "gamma = 0.98\n",
    "device = torch.device(\"cuda\") if torch.cuda.is_available() else torch.device(\n",
    "    \"cpu\")\n",
    "\n",
    "env_name = 'CartPole-v0'\n",
    "env = gym.make(env_name)\n",
    "state_dim = env.observation_space.shape[0]\n",
    "action_dim = env.action_space.n\n",
    "agent = REINFORCE(state_dim, hidden_dim, action_dim, lr, gamma, device)\n",
    "\n",
    "return_list = []\n",
    "for i in range(10):\n",
    "    with tqdm(total=int(num_episodes / 10),                desc='Iteration %d' % i) as pbar:\n",
    "        for i_episode in range(int(num_episodes / 10)):\n",
    "            episode_return = 0\n",
    "            transition_dict = {'states': [], 'actions': [], 'next_states': [], 'rewards': [], 'dones': []}\n",
    "            state = env.reset()\n",
    "            done = False\n",
    "            while not done:\n",
    "                action = agent.take_action(state)\n",
    "                next_state, reward, done, _ = env.step([action])\n",
    "                transition_dict['states'].append(state)\n",
    "                transition_dict['actions'].append(action)\n",
    "                transition_dict['next_states'].append(next_state)\n",
    "                transition_dict['rewards'].append(reward)\n",
    "                transition_dict['dones'].append(done)\n",
    "\n",
    "                state = next_state\n",
    "                episode_return += reward\n",
    "            return_list.append(episode_return)\n",
    "            agent.update(transition_dict)\n",
    "            if (i_episode + 1) % 10 == 0:\n",
    "                pbar.set_postfix({\n",
    "                    'episode':\n",
    "                    '%d' % (num_episodes / 10 * i + i_episode + 1),\n",
    "                    'return':\n",
    "                    '%.3f' % np.mean(return_list[-10:])\n",
    "                })\n",
    "            pbar.update(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "ename": "ValueError",
     "evalue": "operands could not be broadcast together with shapes (0,) (4,) ",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-5-05dc87a99c76>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      6\u001b[0m \u001b[0mplt\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshow\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      7\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 8\u001b[1;33m \u001b[0mmv_return\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mrl_utils\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmoving_average\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mreturn_list\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m9\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      9\u001b[0m \u001b[0mplt\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mplot\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mepisodes_list\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmv_return\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     10\u001b[0m \u001b[0mplt\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mxlabel\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'Episodes'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mh:\\OneDrive - lancely\\芷瀚同步\\qt\\python\\整活\\动手强化学习\\rl_utils.py\u001b[0m in \u001b[0;36mmoving_average\u001b[1;34m(a, window_size)\u001b[0m\n\u001b[0;32m     26\u001b[0m     \u001b[0mmiddle\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mcumulative_sum\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mwindow_size\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m-\u001b[0m \u001b[0mcumulative_sum\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m-\u001b[0m\u001b[0mwindow_size\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m/\u001b[0m \u001b[0mwindow_size\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     27\u001b[0m     \u001b[0mr\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0marange\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mwindow_size\u001b[0m \u001b[1;33m-\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m2\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 28\u001b[1;33m     \u001b[0mbegin\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcumsum\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0ma\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m:\u001b[0m\u001b[0mwindow_size\u001b[0m \u001b[1;33m-\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;36m2\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m/\u001b[0m \u001b[0mr\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     29\u001b[0m     \u001b[0mend\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcumsum\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0ma\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m-\u001b[0m\u001b[0mwindow_size\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m-\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;36m2\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m/\u001b[0m \u001b[0mr\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m-\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     30\u001b[0m     \u001b[1;32mreturn\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mconcatenate\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mbegin\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmiddle\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mend\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mValueError\u001b[0m: operands could not be broadcast together with shapes (0,) (4,) "
     ]
    }
   ],
   "source": [
    "episodes_list = list(range(len(return_list)))\n",
    "plt.plot(episodes_list, return_list)\n",
    "plt.xlabel('Episodes')\n",
    "plt.ylabel('Returns')\n",
    "plt.title('REINFORCE on {}'.format(env_name))\n",
    "plt.show()\n",
    "\n",
    "mv_return = rl_utils.moving_average(return_list, 9)\n",
    "plt.plot(episodes_list, mv_return)\n",
    "plt.xlabel('Episodes')\n",
    "plt.ylabel('Returns')\n",
    "plt.title('REINFORCE on {}'.format(env_name))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "5a98bc0709bb551cb0b40a68bfcb118c11ed773779c4b4ca5eb3852e4a8f5446"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
